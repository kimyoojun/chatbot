{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 import library\n",
    "import dis\n",
    "import imageio\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# matplotlib style 설정\n",
    "# matplotlib.style.use(\"ggplot\")\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 variable setting\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "sample_size = 64\n",
    "nz = 128\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 mnist download\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"pytorch/data2\", train=True, transform=transform, download=True\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz):\n",
    "        super().__init__()\n",
    "        self.nz = nz\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.nz, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Descriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_input = 784\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.n_input, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.2)\n",
      "    (4): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2)\n",
      "    (6): Linear(in_features=1024, out_features=784, bias=True)\n",
      "    (7): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "    (8): Dropout(p=0.3, inplace=False)\n",
      "    (9): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 6 object setting\n",
    "generator = Generator(nz).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 loss function, optimizer setting\n",
    "optim_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optim_d = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "losses_g = []\n",
    "losses_d = []\n",
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 image save function\n",
    "def save_generator_image(image, path):\n",
    "    save_image(image, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 descriminator train function\n",
    "def train_discriminator(optimizer, data_real, data_fake):\n",
    "    b_size = data_real.size(0)\n",
    "    real_label = torch.ones(b_size, 1).to(device)\n",
    "    fake_label = torch.zeros(b_size, 1).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output_real = discriminator(data_real)\n",
    "    loss_real = criterion(output_real, real_label)\n",
    "    output_fake = discriminator(data_fake)\n",
    "    loss_fake = criterion(output_fake, fake_label)\n",
    "    loss_real.backward()\n",
    "    loss_fake.backward()\n",
    "    optimizer.step()\n",
    "    return loss_real + loss_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 generator train function\n",
    "def train_generator(optimizer, data_fake):\n",
    "    b_size = data_fake.size(0)\n",
    "    real_label = torch.ones(b_size, 1).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = discriminator(data_fake)\n",
    "    loss = criterion(output, real_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 200\n",
      "Generator loss: 1.58750820, Discriminator loss: 0.81696784\n",
      "Epoch 1 of 200\n",
      "Generator loss: 2.84170175, Discriminator loss: 1.46880805\n",
      "Epoch 2 of 200\n",
      "Generator loss: 4.33265638, Discriminator loss: 0.48661804\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\chatbot\\python\\pytorch\\torch48.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/chatbot/python/pytorch/torch48.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         loss_d \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_discriminator(optim_d, image, data_fake)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/chatbot/python/pytorch/torch48.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     data_fake \u001b[39m=\u001b[39m generator(torch\u001b[39m.\u001b[39mrandn(b_size, nz)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/chatbot/python/pytorch/torch48.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     loss_g \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_generator(optim_g, data_fake)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/chatbot/python/pytorch/torch48.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     idxh \u001b[39m=\u001b[39m idx\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/chatbot/python/pytorch/torch48.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m generated_image \u001b[39m=\u001b[39m generator(torch\u001b[39m.\u001b[39mrandn(sample_size, nz)\u001b[39m.\u001b[39mto(device))\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\n",
      "\u001b[1;32mc:\\chatbot\\python\\pytorch\\torch48.ipynb Cell 11\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/chatbot/python/pytorch/torch48.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m output \u001b[39m=\u001b[39m discriminator(data_fake)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/chatbot/python/pytorch/torch48.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, real_label)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/chatbot/python/pytorch/torch48.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/chatbot/python/pytorch/torch48.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/chatbot/python/pytorch/torch48.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 11 model training\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_g = 0.0\n",
    "    loss_d = 0.0\n",
    "    idxh = 0\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        image, _ = data\n",
    "        image = image.to(device)\n",
    "        b_size = len(image)\n",
    "        for step in range(k):\n",
    "            data_fake = generator(torch.randn(b_size, nz).to(device).detach())\n",
    "            data_real = image\n",
    "            loss_d += train_discriminator(optim_d, image, data_fake)\n",
    "        data_fake = generator(torch.randn(b_size, nz).to(device).detach())\n",
    "        loss_g += train_generator(optim_g, data_fake)\n",
    "        idxh = idx\n",
    "\n",
    "    generated_image = generator(torch.randn(sample_size, nz).to(device)).cpu().detach()\n",
    "    generated_image = make_grid(generated_image)\n",
    "    save_generator_image(generated_image, f\"pytorch/data2/generated_images_{epoch}.png\")\n",
    "    images.append(generated_image)\n",
    "    epoch_loss_g = loss_g / idxh\n",
    "    epoch_loss_d = loss_d / idxh\n",
    "    losses_g.append(epoch_loss_g)\n",
    "    losses_d.append(epoch_loss_d)\n",
    "\n",
    "    print(f\"Epoch {epoch} of {epochs}\")\n",
    "    print(f\"Generator loss: {epoch_loss_g:.8f}, Discriminator loss: {epoch_loss_d:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "losses_g = [fl.item() for fl in losses_g]\n",
    "plt.plot(losses_g, label='Generator loss')\n",
    "losses_d = [f2.item() for f2 in losses_d]\n",
    "plt.plot(losses_d, label='Discriminator Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_images = generator(torch.randn(b_size, nz).to(device))\n",
    "for i in range(10):\n",
    "    fake_images_img = np.reshape(fake_images.data.cpu().numpy()[i],(28,28))\n",
    "    plt.imshow(fake_images_img, cmap='gray')\n",
    "    plt.savefig('pytorch/data2/fake_images_img' + str(i) + '.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
